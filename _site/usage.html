<h1>Image Model Distributed Training</h1>

<h2>PyTorch</h2>

<p><em>efficientnet_pytorch</em> library downloads the models in .cache/torch/checkpoints. In case your machine has no internet connection, make sure to add the models manually.</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5</pre></td><td class="code"><pre>python -m torch.distributed.launch eff_big_training_distributed.py \
	-n 1 -g 4 -nr 0 \
	--epochs 20 \
	--eff_model b0 \
	--load_path /gpfs/scratch/bsc31/bsc31275/<span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>
    <p><code class="highlighter-rouge">n</code>: number of nodes</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">g</code>: number of gpus in each node</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">nr</code>: the rank of the current node within all the nodes</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">epochs</code>: training number of epochs</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">eff_model</code>: EfficientNet model</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">load_path</code>: path where datasets are stored</p>
  </li>
</ul>

<h2>TensorFlow</h2>

<p><em>efficientnet.tfkeras</em> library downloads the models in .keras/models.</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>python distr_effnet_shear.py --image_model 0 --optimizer sgd --epochs 20<span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>
    <p><code class="highlighter-rouge">image_model</code>: : EfficientNet model</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">optimizer</code>: optimizer to be used</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">epochs</code>: training number of epochs</p>
  </li>
</ul>

<h1>Text Model (PyTorch)</h1>

<p>pytorch_transformers library downloads the models in .cache/torch/pytorch_transformers. BERT training is simply done by running <code class="highlighter-rouge">python main.py</code>. To get the ensemble results run <code class="highlighter-rouge">python ensemble.py</code>.</p>

<h3 style="margin-top:80px;">Also see <strong>README.md</strong> <a href="https://github.com/javiferran/document-classification/blob/master/README.md"><em>on github!</em></a></h3>

