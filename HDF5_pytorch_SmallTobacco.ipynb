{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import numpy\n",
    "import h5py\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import os\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, Sentence\n",
    "\n",
    "import tensorwatch as tw\n",
    "\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "\n",
    "\n",
    "# initialize the word embeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              flair_embedding_backward,\n",
    "                                              flair_embedding_forward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation with image directory, image -> 'RGB' -> transformed to Mobilenetv2 input, Ocr,\n",
    "# Class and Segmentation\n",
    "\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])}\n",
    "#Independent train and test transformations can be done\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, hdf5_file, data_transforms):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.h5_file = h5py.File(hdf5_file, \"r\")\n",
    "        self.data = self.h5_file.get('train_img')\n",
    "        self.target = self.h5_file.get('train_labels')\n",
    "        self.ocr = self.h5_file.get('train_ocrs')\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.data[idx,:,:,:],\n",
    "        img = Image.fromarray(img[0].astype('uint8'), 'RGB')\n",
    "        #doc_class = torch.from_numpy(self.target[idx,:,:,:]).float()\n",
    "        doc_class = self.target[idx]\n",
    "        doc_class = doc_class.astype(np.uint8)\n",
    "        doc_class = torch.tensor(doc_class)\n",
    "        \n",
    "        ocr_text = self.ocr[idx]\n",
    "        \n",
    "\n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "                image = self.data_transforms(img)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\")\n",
    "        \n",
    "\n",
    "        ocr = ocr_text #ocr_text\n",
    "\n",
    "        sentence = Sentence(ocr)\n",
    "\n",
    "        #flair_embedding_fast = FlairEmbeddings('multi-forward-fast')\n",
    "        #flair_embedding_fast.embed(sentence)\n",
    "\n",
    "        flair_embedding_forward.embed(sentence)\n",
    "\n",
    "\n",
    "\n",
    "        #flair_embedding_fast.embed(sentence)\n",
    "        counter = 0\n",
    "        for token in sentence:\n",
    "            #print(token)\n",
    "            token_embedding = token.embedding\n",
    "            token_embedding = token_embedding.unsqueeze(0)\n",
    "            #print(token_embedding)\n",
    "            #print(token_embedding.shape)\n",
    "            if counter == 0:\n",
    "                prev_token_embedding = token_embedding\n",
    "            if counter != 0:\n",
    "                prev_token_embedding = torch.cat((prev_token_embedding, token_embedding),0)\n",
    "            counter += 1\n",
    "\n",
    "        \n",
    "        \n",
    "        #print('Sentence embedded size',prev_token_embedding.shape)\n",
    "            \n",
    "        #document_embeddings.embed(sentence_doc)\n",
    "        \n",
    "        #print('Document embedding', sentence_doc.get_embedding().shape)\n",
    "        \n",
    "        #prev_token_embedding = sentence_doc.get_embedding()\n",
    "        \n",
    "        sample = {'image': image, 'class': doc_class, 'ocr': prev_token_embedding}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dataset = H5Dataset(hdf5_file='./HDF5_files/hdf5_10.hdf5', data_transforms=data_transforms['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(h5_dataset, batch_size=1,\n",
    "                        shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        #self.args = args\n",
    "        \n",
    "        #V = args.embed_num\n",
    "        D = 2048 #embed_dim, 4196 for doc_embeddings\n",
    "        C = 10 #class_num\n",
    "        Ci = 1\n",
    "        Co = 100 #kernel_num -> number of kernel with the same size\n",
    "        Ks = [3,4,5] #kernel_sizes -> size = number of words\n",
    "\n",
    "        #self.embed = nn.Embedding(V, D)\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        \n",
    "        #Output will be size (1,Ks*Co) -> Maxpool will get one Ä‰ value =  max(c_1,c_2...), where c_i is\n",
    "        #the result of the convolution operation of the kernel over the input\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "        #if self.args.static:\n",
    "            #x = Variable(x)\n",
    "        #print('CNN Text entry',x.shape)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        #print('unsqueeze',x.shape)\n",
    "        \n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        \n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        \n",
    "\n",
    "        x = torch.cat(x, 1) #[1,100] + [1,100] + [1,100] = [1,300]\n",
    "        \n",
    "        #print('After cat', x.shape)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = x\n",
    "        #logit = self.fc1(x)  # (N, C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mega NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenetv2_19']\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, expansion=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes*expansion, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes*expansion)\n",
    "        self.conv2 = nn.Conv2d(inplanes*expansion, inplanes*expansion, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False, groups=inplanes*expansion)\n",
    "        self.bn2 = nn.BatchNorm2d(inplanes*expansion)\n",
    "        self.conv3 = nn.Conv2d(inplanes*expansion, planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, text_model, num_classes=16):\n",
    "        self.inplanes = 32\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], stride=1, expansion = 1)\n",
    "        self.layer2 = self._make_layer(block, 24, layers[1], stride=2, expansion = 6)\n",
    "        self.layer3 = self._make_layer(block, 32, layers[2], stride=2, expansion = 6)\n",
    "        self.layer4 = self._make_layer(block, 64, layers[3], stride=2, expansion = 6)\n",
    "        self.layer5 = self._make_layer(block, 96, layers[4], stride=1, expansion = 6)\n",
    "        self.layer6 = self._make_layer(block, 160, layers[5], stride=2, expansion = 6)\n",
    "        self.layer7 = self._make_layer(block, 320, layers[6], stride=1, expansion = 6)\n",
    "        self.conv8 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, bias=False)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.conv9 = nn.Conv2d(1280,num_classes, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "        #Added\n",
    "        #Fully connected\n",
    "        self.fc1 = nn.Linear(310, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        self.text_model = text_model\n",
    "                \n",
    "\n",
    "    #def conv_and_pool(self, x, conv):\n",
    "        #x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        #x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        #return x\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride, expansion):\n",
    "\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride=stride, downsample=downsample, expansion=expansion))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, expansion=expansion))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, x2): #NN input -> Image + ocr text\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv9(x)\n",
    "        \n",
    "        #print('conv9 output', x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        \n",
    "        #print(x.size(0))#1Xnum_classes size\n",
    "        \n",
    "        #print('mobilenet output', x.shape)\n",
    "        \n",
    "        \n",
    "        x2 = self.text_model(x2)\n",
    "        \n",
    "        #print('CNN Text output',x2.shape)\n",
    "        \n",
    "        #print('Text model output (without last layer)', x2.shape)\n",
    "                \n",
    "        x2 = torch.cat((x,x2),1)\n",
    "        \n",
    "        x2 = self.fc1(x2)\n",
    "        \n",
    "        #print('MegaNet output',x2)\n",
    "        \n",
    "        #print('Output shape', x2.shape)\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def mobilenetv2_19(text_model, **kwargs):\n",
    "    \"\"\"Constructs a MobileNetV2-19 model.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(Bottleneck, [1, 2, 3, 4, 3, 3, 1], text_model, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mega NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "text_model = CNN_Text()\n",
    "model = mobilenetv2_19(text_model, num_classes = 10)\n",
    "#print(model)\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/30], loss 2.276034951210022\n",
      "[Epoch 1/30], loss 2.1527021169662475\n",
      "[Epoch 2/30], loss 1.5654648184776305\n",
      "[Epoch 3/30], loss 1.3598836183547973\n",
      "[Epoch 4/30], loss 1.1512081503868103\n",
      "[Epoch 5/30], loss 1.2347265601158142\n",
      "[Epoch 6/30], loss 1.059875726699829\n",
      "[Epoch 7/30], loss 0.8478475093841553\n",
      "[Epoch 8/30], loss 0.8234397888183593\n",
      "[Epoch 9/30], loss 0.7017635345458985\n",
      "[Epoch 10/30], loss 0.6536667108535766\n",
      "[Epoch 11/30], loss 0.6613679885864258\n",
      "[Epoch 12/30], loss 0.6061566114425659\n",
      "[Epoch 13/30], loss 0.4479806900024414\n",
      "[Epoch 14/30], loss 0.43779473304748534\n",
      "[Epoch 15/30], loss 0.3658362865447998\n",
      "[Epoch 16/30], loss 0.3325450658798218\n",
      "[Epoch 17/30], loss 0.2932209730148315\n",
      "[Epoch 18/30], loss 0.27964737415313723\n",
      "[Epoch 19/30], loss 0.2673135280609131\n",
      "[Epoch 20/30], loss 0.2510611772537231\n",
      "[Epoch 21/30], loss 0.23877439498901368\n",
      "[Epoch 22/30], loss 0.2081374168395996\n",
      "[Epoch 23/30], loss 0.1669908046722412\n",
      "[Epoch 24/30], loss 0.1810145616531372\n",
      "[Epoch 25/30], loss 0.15001235008239747\n",
      "[Epoch 26/30], loss 0.17530531883239747\n",
      "[Epoch 27/30], loss 0.19496288299560546\n",
      "[Epoch 28/30], loss 0.1337438106536865\n",
      "[Epoch 29/30], loss 0.10305595397949219\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa34906a76cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#111111\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./snapshot/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "max_epochs = 30\n",
    "optimizer = optimizer_ft\n",
    "batch_size=1\n",
    "running_loss = 0.0\n",
    "steps = 0\n",
    "loss_values = []\n",
    "epoch_values = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    running_loss = 0\n",
    "    # Training\n",
    "    for local_batch in dataloader:\n",
    "        image, ocr_text, labels = Variable(local_batch['image']), Variable(local_batch['ocr']), Variable(local_batch['class'])\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #print(local_batch['image_dir'])\n",
    "\n",
    "        # forward\n",
    "        outputs = model(image, ocr_text)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        #print(preds, labels.long())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        #print(outputs)\n",
    "\n",
    "        # backward + optimize only if in training phase\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #running_loss += loss.data[0]\n",
    "        steps += 1\n",
    "        #if steps % 5 == 0:\n",
    "            #save(model,'./snapshot/', 'model', steps)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    #print(outputs)ack\n",
    "    print('[Epoch {}/{}], loss {}'.format(\n",
    "                      epoch, max_epochs,running_loss/10))\n",
    "        \n",
    "   \n",
    "    loss_values.append(running_loss/10)\n",
    "    epoch_values.append(epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epoch_values, loss_values, '--', color=\"#111111\",  label=\"Training score\")\n",
    "\n",
    "#save(model,'./snapshot/', 'model', steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
